\section{Conclusion and Future Work}

We have presented a new reinforcement-learning-based algorithm for
optimizing a quadrupedal gait for linear speed. We implemented and
tested six learning strategies for parameterized gaits and compared
them to gaits produced by neural networks evolved with the HyperNEAT
generative encoding. Though both methods resulted in an improvement
over the robotâ€™s previous \naive gaits, based on the statistics
collected, RL PoWER has a more elegant and consistent performance.

Though over 900 trials have been made to investigate the applicability
of RL PoWER to quadruped robots. It is difficult to gather the enough
trials that would be necessary to properly rank the methods. One
direction for future work could be to obtain many more trials. But due
to the physical limitations, obtaining One solution to this is
simulation. Because of the low cost of simulation, it would produce
the necessary volume of trials to allow the learning methods to be
effective, and the hardware trials would serve to continuously ground
and refine the simulator. \cite{glette2012evolution-of-locomotion-in-a-simulated}

One guess led by this study is that for feedback oriented tasks,
reinforcement learning methods are more fit in natural. Despite the
complexities of HyperNEAT, a simpler algorithm on the code level
delivered better performance in general.  Also, evolvable spline
interpolation is shown to be simple and representationally powerful at
the same time. Evolvable splines can serve as a general representation for various other
learning problems.

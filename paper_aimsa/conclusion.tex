\section{Conclusion and Future Work}

We have presented results from a recently introduced reinforcement-learning-based algorithm for
optimizing a quadrupedal gait for linear speed, tested on a physical robot.  We implemented the algorithm, Policy learning by Weighting
Exploration with the Returns (RL PoWER), for parameterized gaits and compared
it to gaits produced by neural networks evolved with the HyperNEAT
generative encoding. Though both methods resulted in an improvement
over the robot's previous \naive gaits, based on the statistics
collected, RL PoWER has a more elegant and consistent performance.

Over 900 trials have been made to investigate the applicability
of RL PoWER to quadruped robots. It is difficult to gather the enough
trials that would be necessary to properly rank the methods. One
direction for future work could be to obtain many more trials. But due
to the physical limitations, obtaining one solution to this is
simulation. Because of the low cost of simulation, it would produce
the necessary volume of trials to allow the learning methods to be
effective, and the hardware trials would serve to continuously ground
and refine the simulator. 

One view supported by this study is that for feedback oriented tasks,
reinforcement learning methods are more fit and natural. Despite the
complexities of HyperNEAT, a structurally simpler algorithm delivered better performance in general.  Also, evolvable spline
interpolation is shown to be simple and representationally powerful at
the same time. Evolvable splines can serve as a general representation for various other
learning problems. \editbox{This last paragraph is better, but it still needs to be rewritten to be more readable. It's the last thing people will read in the paper, so it should be strong.}

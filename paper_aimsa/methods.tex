\section{Methods}



\subsection{Parameterized Gaits by RL\_PoWER}

Here we used an RL approach to change the complexity of the policy
representation dynamically while the trial is running. In Petar et alâ€™
s studies on reducing energy consumption for bipedal robots [xx], a
mechanism that can evolve the policy parameterization was used. The
method starts from a very simple parameterization and gradually
increases its representational capability. The method tested to be
able to generate an adaptive policy parameterization that can
accommodate increasingly more complex policies. In the work described
in [xx], the policy generated by this approach can reach the global
optimum at a faster rate. Also found is that the chance of converging
to a suboptimal solution is reduced, because in the lower-dimensional
representation the effect is less exhibited.



\subsection{Spline policy representation}

The simplest model with back-compatibility is geometric
splines. For a given model f(x) with K knots, we can preserve the
exact shape of the generated curve while adding extra knots to the
original spline. Say, if we put one additional knot between every two
consecutive knots of the original spline, we end up with a 2K - 1
knots and a spline that has the same shape as the original one. In
order to do this, we need to define an algorithm for evolving the
parameterization from $K$ to $L$ knots ($L > K$), which is formulated in
Algorithm \edit{proper reference}.  Without loss of generality, the policy parameters are
normalized into $[0, 1]$, and appropriately scaled/shifted as necessary
later upon use.

\figp{powerSplinesExample}{.75}{Example figure... from Petar's paper...}

[pic]Fig. 2 illustrates the process of using spline
representation for the evolving policy parameterization. Fig. 1 shows
an example for a reinforcement learning process using evolving policy
parameterization to approximate an unknown function. Petar et
alIntegrating evolving policy parameterization with
Hill-climbing(RL?)Kober et al proposed (Kober et al) a RL algorithm
called Policy learning by Weighting Exploration with the
Returns(PoWER), which is based on Expectation-Maximization algorithm
(EM). Our proposed technique for evolving the policy parameterization
are combined with this EM-based RL algorithm PoWER [xx], due to its
relatively fewer parameters that need tuning. Also because PoWER
demonstrated high performance in tasks learned directly on real
robots, such as pendulum swing-up and pancake flipping task [9].We
evolved the policy parameterization only on those past trials ranked
the highest by the importance sampling technique used by the PoWER
algorithm(). The intuition behind is that high-ranked
parameterizations have more potential to perform even better in the
future. Besides, evolving all the parameterizations requires
significantly more efforts. (Bongard et al.,2006) Since our experiment
is done on physical robot, explore all the variations of every
parameterization is not practical. A promising extension can be, using
physical simulator for the robot, to further explore the
parameterization space by evolving more parameterizations. Gaits of
RL\_POWER:For the experiment, we have 3 knots for each spline and 8
splines in total for the servos. The servo in the hip is not used in
our experiment. Previous work has verified that quadruped gaits
perform better when they are coordinated (Clune et al., 2009a, 2011;
Valsalam and Miikkulainen, 2008). So the gaits For each set of
splines, we calculate its corresponding parameterized gait within a
time cycle. Given the pattern of one unit cycle, then apply the same
pattern to every cycle throughout the whole period of
trial. Specifically, each spline is stretched to servo positions as
following: rescale for one cycle: Unit Cycle :s(si).cyclePos =
s(si).YY .* posRange + posMin; s(si).cyclePos = s(si).cyclePos(:);
Repeat cycle, Cycle[i] = UnitCycle \% the last pos in a cycle is the
same as the first pos in the next cycle, from [pic for one gait]

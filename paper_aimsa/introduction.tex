\section{Introduction}

\figp{quadratotWhiteBooties}{.75}{}

Various learning algorithms have been proved to be effective for
legged robots. Algorithms such as HyperNEAT \cite{yosinski2011evolving-robot-gaits},
GA \cite{chernova2004an-evolutionary-approach-to-gait} and others \cite{hornby2005autonomous-evolution-of-dynamic} \cite{zykov2004evolving-dynamic-gaits} \cite{tellez2006evolving-the-walking-behaviour} \cite{valsalam2008modular-neuroevolution-for-multilegged} have been tested to be effective for automatic learning gaits for robots. Despite of the competetive performance, a major
task lies in tuning the parameters for
these evolutionary algorithms \cite{kormushev2011bipedal-walking-energya}. Here we present a
different way for learning gaits using reinforcement learning and expectation maximization, called RL PoWER. In our
experiment, the main focus of the research is on the applicability of RL PoWER to quadruped robot gait learning. Another motivation is to compare the state-of-art neural network algorithm, HyperNEAT, with the performance of our proposed method.

\subsection{Problem Definition}
The gait learning problem is defined to find a
gait that maximizes some specific metric. Mathematically, we define a
gait as a function that specifies a vector of commanded motor
positions for a robot over time. Gaits without feedback --- also
called open-loop gaits --- can be defined as 

\be\vec{x} = g(t)\ee

According to this definition, open-loop gaits are deterministic. One
particular gait should behave exactly the same when it is run from
trial to trial. However, the actual robot motion and fitness measured
will vary due to the errors and uncertainty of the real world physics. In our
trials, the gaits generated were sent to the robot and
executed in an open loop manner. We will present the performance comparison between
RL PoWER and HyperNEAT.  the latter of which have the best in details. The metric used will be
described later.

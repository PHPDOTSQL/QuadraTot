\subsection{Related Work}



Various machine learning techniques have proven to be effective at generating gaits for legged robots. Kohl and Stone presented a policy gradient reinforcement learning approach for generating a fast walk on legged robots\cite{kohl}. We experiment with this method to create a walk for our robot (called Policy Gradient Descent, described below). Others have evolved gaits for legged robots, producing competitive results~\cite{chernova2005evolutionary, hornby2005autonomous, zykov, clune2009evolving, clune2011performance, clune2009hybrid, clune2009sensitivity, tellez2006evolving, valsalam2008modular}. In fact, an evolved gait was used in the first commercially-available version of Sony's AIBO robot~\cite{hornby2005autonomous}. Except for work with HyperNEAT~\cite{clune2009evolving, clune2011performance, clune2009hybrid, clune2009sensitivity}, the previous evolutionary approaches have helped evolution 'see' the regularity of the problem by manually decomposing the task. In other words, experimenters have to choose which legs should be coordinated, or otherwise facilitate the coordination of robotic legs. Part of the motivation of this paper is to see if the space of regularities can be explored automatically, which has previously performed well~\cite{valsalam2008modular}, and to perform a direct comparison on the same robot to HyperNEAT.  


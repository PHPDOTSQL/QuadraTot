% !TEX root =main.tex

\subsubsection{HyperNEAT:}

HyperNEAT is an algorithm for evolving artificial neural networks (ANNs)~\cite{stanley2009hypercube}. It has been repeatedly described in detail~\cite{stanley2009hypercube,gauci2007generating,clune2011performance},  so here we provide only a summary. Instead of directly encoding each ANN weight individually on the genome, in HyperNEAT the genome is a compositional pattern producing network (CPPN)~\cite{stanley2007CPPN}. The CPPN specifies the weights in a similar way to how natural organisms develop. In nature, phenotypic attributes are specified as a function of their geometric location, and such positional information is conveyed through chemical morphogen gradients~\cite{carroll2005endless}. For example, the concentration of one chemical could indicate the position along the head-to-tail axis and another chemical in bands could indicate if a cell is in an odd- or even-numbered segment. Based on the relative concentrations of these chemicals, a cell can know where it is geometrically and, thus, what type of cell to become~\cite{carroll2005endless}. 

With CPPNs this process is abstracted as a network of math functions that operate in a Cartesian geometric space. The coordinates of phenotypic elements are provided as inputs to the CPPN and the outputs specify phenotypic traits. For example, when CPPNs encode 2D pictures, the coordinates of each pixel are iteratively input into the genome and the output is the grayscale value at that coordinate~\cite{secretan2011picbreeder}. Because a CPPN network is composed of math functions, those functions can create geometric regularities in the phenotype. For example, a Gaussian function of an axis can provide symmetry (e.g. left-right), and a repeating function (e.g. sine) of an axis could provide repetition (e.g segmentation). Both 2D pictures and 3D objects evolved with CPPNs look like natural and engineered objects, and contain complex regularities, such as symmetries and repeated motifs, with and without variation~\cite{secretan2011picbreeder,clune2011objects}. 

%\figp{hyperneat_bug_example}{.8}{Compositional Pattern Producing Networks (CPPNs). CPPN  genomes use mathematical functions to generate phenotypic regularities, such as symmetry and repetitions. Figure adapted from Stanley 2007~\cite{stanley2007CPPN}.}


In HyperNEAT, CPPNs encode the weights of the connections between neurons as a function of the geometric locations of those neurons~(\figref{hyperneatExplanation}). The Cartesian coordinates of the two neurons at the end of each connection are input into the CPPN, and the output is the weight of that connection. If the output is smaller than a threshold, the weight is set to zero, functionally removing the connection. The process is repeated for each possible connection. Just as in 2D pictures and 3D objects, the CPPN can create complex, regular, geometric patterns (e.g. left-right symmetry or repeated modules), but in this case the patterns are in the weights of the neural network~\cite{clune2011performance}. The neural regularities produced by HyperNEAT enable significantly improved performance on problems that are regular~\cite{clune2011performance,stanley2009hypercube}, including evolving quadruped gaits in simulation~\cite{clune2011performance,clune2009evolving,clune2009sensitivity}. 

In HyperNEAT, the CPPN genomes evolve via the Neuroevolution of Augmenting Topologies (NEAT) algorithm~\cite{stanley2002evolving}, which has three major components. 
First, NEAT starts with small genomes that encode simple networks and complexifies them via mutations that add nodes and links to the networks. Second, NEAT has a fitness-sharing system that preserves diversity and allows for new innovations to be tuned by evolution before competing them against more adapted rivals. 
Third, historical information is recorded that facilitates crossover in a way that is effective, yet avoids the need for expensive topological analysis. A full explanation of NEAT can be found in Stanley and Miikkulainen 2002~\cite{stanley2002evolving}. 

In this study, the ANN inputs, outputs, activation functions, and the size of the hidden layer are the same as in Yosinski et al.~\cite{yos:clune}. 
The ANN had a fixed topology of three \begin{math}3\times4\end{math} Cartesian grids of nodes for the input, hidden, and output layers. 
The inputs to the substrate were the angles requested in the prevous time step for each of the 9 joints of the robot and a sine and cosine wave to facilitate periodic motion. 
The outputs of the substrate at each time step were nine numbers (for each joint) in the range [-1, 1] which were scaled to the range [0, 1023], the allowable ranges for the servos. 
As in Yosinski et al.~\cite{yos:clune}, we generated pseudo-positions at 160Hz and then downsampled over consecutive blocks of four time steps to obtain the actual commanded positions at 40Hz; this reduced the number of gaits which commanded switches from extreme negative to extreme positive numbers at 40Hz, which overly taxed the servos.

\figp{hyperneatExplanation}{1}{A CPPN specifying a neural network. In HyperNEAT, weights are a function of the Cartesian coordinates of the source and target node for each connection. All pairwise combinations of source and target node coordinates are iteratively passed into a CPPN to determine the weight of each ANN link. Figure from Clune et al.~\cite{clune2011performance}.}


